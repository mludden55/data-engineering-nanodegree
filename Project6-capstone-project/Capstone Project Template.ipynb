{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlsxwriter in /opt/conda/lib/python3.6/site-packages (1.2.2)\n",
      "Requirement already satisfied: ijson in /opt/conda/lib/python3.6/site-packages (2.5.1)\n",
      "Requirement already satisfied: pandas==0.24.0 in /opt/conda/lib/python3.6/site-packages (0.24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas==0.24.0) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas==0.24.0) (2017.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.6/site-packages (from pandas==0.24.0) (1.12.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas==0.24.0) (1.11.0)\n",
      "Initialization Complete!\n"
     ]
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import sys\n",
    "!{sys.executable} -m pip install xlsxwriter\n",
    "!{sys.executable} -m pip install ijson\n",
    "!{sys.executable} -m pip install pandas==0.24.0\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "import xlsxwriter\n",
    "import configparser\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pprint\n",
    "import csv\n",
    "import logging \n",
    "\n",
    "# Create and configure logger \n",
    "logging.basicConfig(filename=\"capstone_project_template.log\", \n",
    "                    format='%(asctime)s %(message)s', \n",
    "                    filemode='w') \n",
    "  \n",
    "# Creating logging object \n",
    "logger=logging.getLogger() \n",
    "  \n",
    "# Setting the threshold of logger to DEBUG \n",
    "logger.setLevel(logging.DEBUG) \n",
    "\n",
    "print(\"Initialization Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables for extracting data to\n",
    "%run -i 'create_tables.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-03f57dfc": {
       "style": "primary"
      }
     }
    }
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "The goal of this project is to take various sources of data related to immigration to the United States (US) and create an ETL pipeline to create a single database for further analysis. The database can then be easily queried to analyze subjects such as do immigrants tend to migrate to warmer destinations, larger cities, etc.  \n",
    "\n",
    "#### Describe and Gather Data \n",
    "The datasources include:\n",
    "1. Immigration data - includes detailed information about immigrants to the United States.\n",
    "2. Temperature data - includes temperature information. I filtered this to only include information about US cities.\n",
    "3. Airport data - includes basic information about location and type of airport. I filtered this to only include information about US cities.\n",
    "4. Demographics data - includes information about household size and types of members, including foreign born statistics. The original dataset only includes information for US cities.\n",
    "\n",
    "All of the data was provided in the project template. I modified the original us-cities-demographics.csv file to a json file so that I could practice working with different types of data.\n",
    "\n",
    "I used the Immigration dataset as my fact table, with the remaining tables being dimension tables.  I used Pandas and Psycopg to read in the data, display header rows and then insert into the Postgres database.\n",
    "\n",
    "The Immigration dataset is quite large.  On my initial passes, I utilized much smaller subsets of data to create the ETL processes.  I left code that I used for creating these smaller subsets, but commented out for the final project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-03f57dfc": {
       "style": "primary"
      }
     }
    }
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "The following cells read in various data files and displays the first 5 rows of data.\n",
    "\n",
    "In looking at the first few rows, I determined that the following issues should be addressed:  \n",
    "1. Null data in certain columns.\n",
    "2. Data that was not about the 50 \"primary\" US states.\n",
    "3. For immigration data, the month of June had a number of extra columns that were not in any of the other monthly files.  \n",
    "\n",
    "#### Cleaning Steps\n",
    "1. In cases where the null data was deemed critical I simply skipped that row of data so it is not included in the final database.\n",
    "2. I also filtered out any data that was not relevant to the US.\n",
    "3. For immigration data, I only read columns that were in all monthly files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config file for connecting to database\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "conn = config['CLUSTER']['HOST_POS']\n",
    "\n",
    "# Below is for DataFrame.to_sql\n",
    "engine = create_engine(conn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read airport code data and display first 5 rows\n",
    "#df_airport = pd.read_csv('airport-codes_csv.csv', nrows=10000)\n",
    "df_airport = pd.read_csv('airport-codes_csv.csv')\n",
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasetid': 'us-cities-demographics',\n",
      " 'fields': {'average_household_size': 2.73,\n",
      "            'city': 'Newark',\n",
      "            'count': 76402,\n",
      "            'female_population': 143873,\n",
      "            'foreign_born': 86253,\n",
      "            'male_population': 138040,\n",
      "            'median_age': 34.6,\n",
      "            'number_of_veterans': 5829,\n",
      "            'race': 'White',\n",
      "            'state': 'New Jersey',\n",
      "            'state_code': 'NJ',\n",
      "            'total_population': 281913},\n",
      " 'record_timestamp': '1969-12-31T17:00:00-07:00',\n",
      " 'recordid': '85458783ecf5da6572ee00e7120f68eff4fd0d61'}\n",
      "{'datasetid': 'us-cities-demographics',\n",
      " 'fields': {'average_household_size': 2.4,\n",
      "            'city': 'Peoria',\n",
      "            'count': 1343,\n",
      "            'female_population': 62432,\n",
      "            'foreign_born': 7517,\n",
      "            'male_population': 56229,\n",
      "            'median_age': 33.1,\n",
      "            'number_of_veterans': 6634,\n",
      "            'race': 'American Indian and Alaska Native',\n",
      "            'state': 'Illinois',\n",
      "            'state_code': 'IL',\n",
      "            'total_population': 118661},\n",
      " 'record_timestamp': '1969-12-31T17:00:00-07:00',\n",
      " 'recordid': 'a5ad84bdb4d72688fb6ae19a8bee43bcb01f9fea'}\n",
      "{'datasetid': 'us-cities-demographics',\n",
      " 'fields': {'average_household_size': 2.77,\n",
      "            'city': \"O'Fallon\",\n",
      "            'count': 2583,\n",
      "            'female_population': 43270,\n",
      "            'foreign_born': 3269,\n",
      "            'male_population': 41762,\n",
      "            'median_age': 36.0,\n",
      "            'number_of_veterans': 5783,\n",
      "            'race': 'Hispanic or Latino',\n",
      "            'state': 'Missouri',\n",
      "            'state_code': 'MO',\n",
      "            'total_population': 85032},\n",
      " 'record_timestamp': '1969-12-31T17:00:00-07:00',\n",
      " 'recordid': 'c54cd5021a16eb5f7b83987742bd495229b2155e'}\n",
      "{'datasetid': 'us-cities-demographics',\n",
      " 'fields': {'average_household_size': 2.48,\n",
      "            'city': 'Hampton',\n",
      "            'count': 70303,\n",
      "            'female_population': 70240,\n",
      "            'foreign_born': 6204,\n",
      "            'male_population': 66214,\n",
      "            'median_age': 35.5,\n",
      "            'number_of_veterans': 19638,\n",
      "            'race': 'Black or African-American',\n",
      "            'state': 'Virginia',\n",
      "            'state_code': 'VA',\n",
      "            'total_population': 136454},\n",
      " 'record_timestamp': '1969-12-31T17:00:00-07:00',\n",
      " 'recordid': '914487cd3d80f15f9eacf24bf26fe28d518b670f'}\n",
      "{'datasetid': 'us-cities-demographics',\n",
      " 'fields': {'average_household_size': 2.29,\n",
      "            'city': 'Lakewood',\n",
      "            'count': 33630,\n",
      "            'female_population': 76576,\n",
      "            'foreign_born': 14169,\n",
      "            'male_population': 76013,\n",
      "            'median_age': 37.7,\n",
      "            'number_of_veterans': 9988,\n",
      "            'race': 'Hispanic or Latino',\n",
      "            'state': 'Colorado',\n",
      "            'state_code': 'CO',\n",
      "            'total_population': 152589},\n",
      " 'record_timestamp': '1969-12-31T17:00:00-07:00',\n",
      " 'recordid': 'a324ac81d2bed29a4b90d74c839e1698e9d06328'}\n"
     ]
    }
   ],
   "source": [
    "# Read demographics data and output the first 5 rows\n",
    "# Note that I converted original project csv file to json in order to test working with different types of data\n",
    "counter = 0\n",
    "\n",
    "# Read demographics data and display first 5 rows\n",
    "with open('us-cities-demographics.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    for row in data:\n",
    "        counter += 1\n",
    "        if counter > 5:\n",
    "            break\n",
    "        pprint.pprint(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read temperature data and display first 5 rows\n",
    "#df_temperature = pd.read_csv(\"../../data2/GlobalLandTemperaturesByCity.csv\", nrows=48000)\n",
    "df_temperature = pd.read_csv(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "\n",
    "# Display first 5 rows\n",
    "df_temperature.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47555</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>2.101</td>\n",
       "      <td>3.217</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47556</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>6.926</td>\n",
       "      <td>2.853</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47557</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>10.767</td>\n",
       "      <td>2.395</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47558</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>17.989</td>\n",
       "      <td>2.202</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>21.809</td>\n",
       "      <td>2.036</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt  AverageTemperature  AverageTemperatureUncertainty     City  \\\n",
       "47555  1820-01-01               2.101                          3.217  Abilene   \n",
       "47556  1820-02-01               6.926                          2.853  Abilene   \n",
       "47557  1820-03-01              10.767                          2.395  Abilene   \n",
       "47558  1820-04-01              17.989                          2.202  Abilene   \n",
       "47559  1820-05-01              21.809                          2.036  Abilene   \n",
       "\n",
       "             Country Latitude Longitude  \n",
       "47555  United States   32.95N   100.53W  \n",
       "47556  United States   32.95N   100.53W  \n",
       "47557  United States   32.95N   100.53W  \n",
       "47558  United States   32.95N   100.53W  \n",
       "47559  United States   32.95N   100.53W  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the header rows only show Denmark, run our first filter so that only US cities are pulled in\n",
    "df_temperature = df_temperature.query(\"Country == 'United States'\")\n",
    "\n",
    "# Display first 5 rows\n",
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "The primary goal is to assess immigration events, so the immigration table is the center of my star schema, with the remaining tables being dimension tables.\n",
    "\n",
    "The following are tables that were created:  \n",
    "\"\"\"CREATE TABLE fact_immigration(  \n",
    "    immigration_id int,  \n",
    "    cicid int,  \n",
    "    i94yr int NOT NULL,  \n",
    "    i94mon int NOT NULL,  \n",
    "    i94cit int,  \n",
    "    i94res int,  \n",
    "    i94port char(3),  \n",
    "    arrdate int,  \n",
    "    i94mode int,  \n",
    "    i94addr char(3),  \n",
    "    depdate int,  \n",
    "    i94bir int,  \n",
    "    i94visa int,  \n",
    "    count int,  \n",
    "    dtadfile varchar,  \n",
    "    visapost char(3),  \n",
    "    occup char(3),  \n",
    "    entdepa char(1),  \n",
    "    entdepd char(1),  \n",
    "    entdepu char(1),  \n",
    "    matflag char(1),  \n",
    "    biryear int,  \n",
    "    dtaddto varchar,  \n",
    "    gender char(1),  \n",
    "    insnum varchar,  \n",
    "    airline char(3),  \n",
    "    admnum varchar,  \n",
    "    fltno varchar,  \n",
    "    visatype char(3))  \n",
    "\"\"\"  \n",
    "\n",
    "\"\"\"CREATE TABLE dim_temperature(  \n",
    "    dt varchar,  \n",
    "    averageTemperature int,  \n",
    "    averageTemperatureUncertainty int,  \n",
    "    city varchar NOT NULL,  \n",
    "    country varchar NOT NULL,  \n",
    "    longitude char(10) NOT NULL,  \n",
    "    latitude char(10) NOT NULL,  \n",
    "    PRIMARY KEY (dt))  \n",
    "\"\"\"  \n",
    "   \n",
    "\"\"\"CREATE TABLE dim_demographics(  \n",
    "    count int NOT NULL,  \n",
    "    city varchar NOT NULL,  \n",
    "    number_of_veterans int NOT NULL,  \n",
    "    male_population int NOT NULL,  \n",
    "    foreign_born int NOT NULL,  \n",
    "    average_household_size int NOT NULL,  \n",
    "    median_age int NOT NULL,  \n",
    "    state varchar NOT NULL,  \n",
    "    race varchar NOT NULL,  \n",
    "    total_population int NOT NULL,  \n",
    "    state_code char(2) NOT NULL,  \n",
    "    female_population int NOT NULL)  \n",
    "\"\"\"  \n",
    "\n",
    "\"\"\"CREATE TABLE dim_airport(  \n",
    "    ident char(10),  \n",
    "    type varchar,  \n",
    "    name varchar,  \n",
    "    elevation_ft int,  \n",
    "    continent char(2),  \n",
    "    iso_country char(2),  \n",
    "    iso_region char(10),  \n",
    "    municipality varchar,  \n",
    "    gps_code char(10),  \n",
    "    iata_code char(3),  \n",
    "    local_code char(10),  \n",
    "    coordinates varchar)  \n",
    "\"\"\"  \n",
    "\n",
    "\"\"\"CREATE TABLE dim_port(  \n",
    "    abrev varchar,  \n",
    "    port varchar)  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Run Pipelines to Model the Data \n",
    "I filtered the data before inserting into the database. The primary job was to make sure that all data was only for the United States.\n",
    "\n",
    "I also filtered out various rows of data where there were null values in what I considered to be critical columns, such as temperatures.\n",
    "\n",
    "For the demographics data, I filtered out bad data which was primarily related to cities in Puerto Rico. I chose to limit my analysis to the 50 primary states in the US, so I filtered that data out.\n",
    "\n",
    "Finally, I filtered the immigration data so that it only included valid ports. I built a valid set of ports by pulling the information from the I94_SAS_Lables_Descriptions.SAS and creating a csv file. I then queried that file and built temporary variables for filtering the immigration data and also inserted into a database table for analysis purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Data Insert\n",
      "Complete Date Insert\n"
     ]
    }
   ],
   "source": [
    "# Clean the airport dataset created earlier by filtering to only use US airports\n",
    "df_airport = df_airport.query(\"iso_country == 'US'\")\n",
    "\n",
    "print(\"Begin Data Insert\")\n",
    "\n",
    "# Read in the airport file and insert to database\n",
    "df_airport.to_sql(\n",
    "    'dim_airport',\n",
    "    engine,\n",
    "    schema='public',\n",
    "    if_exists='append',\n",
    "    index=False,\n",
    ")\n",
    "    \n",
    "print(\"Complete Date Insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Data Insert\n",
      "Complete Data Insert\n"
     ]
    }
   ],
   "source": [
    "# Create database connections\n",
    "conn_psy = psycopg2.connect(conn)\n",
    "conn_psy.set_session(autocommit=True)\n",
    "cur = conn_psy.cursor()\n",
    "\n",
    "# Create the insert statement\n",
    "dim_demographics_insert = \"\"\"INSERT INTO dim_demographics\n",
    "(average_household_size, city, count, female_population, foreign_born, male_population, median_age, number_of_veterans, race, state, state_code, total_population)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\"\"\"\n",
    "\n",
    "print(\"Begin Data Insert\")\n",
    "\n",
    "# Read the demographics file and insert to database\n",
    "with open('us-cities-demographics.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    for row in data:\n",
    "        try:\n",
    "            cur.execute(dim_demographics_insert, \n",
    "                [row['fields']['average_household_size'], \n",
    "                row['fields']['city'],\n",
    "                row['fields']['count'],\n",
    "                row['fields']['female_population'],\n",
    "                row['fields']['foreign_born'],\n",
    "                row['fields']['male_population'],\n",
    "                row['fields']['median_age'],\n",
    "                row['fields']['number_of_veterans'],\n",
    "                row['fields']['race'],\n",
    "                row['fields']['state'],\n",
    "                row['fields']['state_code'], \n",
    "                row['fields']['total_population']]) \n",
    "        # Certain cities do not have all rows required for insert. Log to file for further analysis.\n",
    "        except:\n",
    "            logger.info(\"Skip City: {}\".format(row['fields']['city'])) \n",
    "\n",
    "print(\"Complete Data Insert\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Data Insert\n",
      "Complete Date Insert\n"
     ]
    }
   ],
   "source": [
    "# Clean the temperature dataset created earlier by filtering out entries with NaN average temperature\n",
    "df_temperature = df_temperature.query(\"AverageTemperature != 'NaN'\")\n",
    "\n",
    "# Sort the data so that we can pull first 120,000 rows in order of descending date\n",
    "df_temperature = df_temperature.sort_values(by='dt', ascending=False)\n",
    "\n",
    "# Filter out first \n",
    "df_temperature = df_temperature.head(120000)\n",
    "    \n",
    "print(\"Begin Data Insert\")\n",
    "\n",
    "df_temperature.to_sql(\n",
    "    'dim_temperature',\n",
    "    engine,\n",
    "    schema='public',\n",
    "    if_exists='append',\n",
    "    index=False,\n",
    ")\n",
    "    \n",
    "print(\"Complete Date Insert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "I ran queries after data loading to verify that each table contained data and also displayed the number of rows in each table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for Temperature table with [(120000,)] records\n",
      "Data quality check passed for Demographics table with [(2875,)] records\n",
      "Data quality check passed for Airport table with [(22757,)] records\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "def check_db_count(table, description):\n",
    "    '''\n",
    "    Input: Table name and description\n",
    "    \n",
    "    Output: Print data quality check\n",
    "    \n",
    "    '''\n",
    "    count_records = \"SELECT count(*) FROM {}\".format(table)\n",
    "    cur.execute(count_records)\n",
    "    result = cur.fetchall()\n",
    "    \n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return\n",
    "\n",
    "# Perform data count check\n",
    "check_db_count(\"dim_temperature\", \"Temperature table\")\n",
    "check_db_count(\"dim_demographics\", \"Demographics table\")\n",
    "check_db_count(\"dim_airport\", \"Airport table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "fact_immigration\n",
    "\n",
    "    immigration_id: Primary key\n",
    "    cicid: Unique key within a month\n",
    "    i94yr: 4 digit year\n",
    "    i94mon: Numeric month, 1-12\n",
    "    i94cit: Immigrant's country of citizenship\n",
    "    i94res: Immigrant's country of residence outside US\n",
    "    i94port: Port of entry\n",
    "    arrdate: Arrival date of immigrant\n",
    "    i94mode: Mode of arrival\n",
    "    i94addr: Address of immigrant in US\n",
    "    depdate: Departure date of immigrant\n",
    "    i94bir: Immigrant's birth date\n",
    "    i94visa: Visa type\n",
    "    count: ???\n",
    "    dtadfile: Dates in the format YYYYMMDD\n",
    "    visapost: Three-letter codes corresponding to where visa was issued\n",
    "    occup: Occupation in US of immigration\n",
    "    entdepa: One-letter arrival code\n",
    "    entdepd: One-letter departure code\n",
    "    entdepu: One-letter update code\n",
    "    matflag: M if the arrival and departure records match\n",
    "    biryear: Year of birth\n",
    "    dtaddto: Date field for when the immigrant is admitted until\n",
    "    gender: Gender\n",
    "    insnum: Immigration and Naturalization Services number\n",
    "    airline: Airline of entry for immigrant\n",
    "    admnum: Admission number\n",
    "    fltno: Flight number\n",
    "    visatype: Visa codes\n",
    "\n",
    "dim_temperature: Provides temperature data. The original dataset was for the whole world, but only US temperatures used for this project.\n",
    "\n",
    "    dt: Date of temperature recording\n",
    "    averageTemperature: Average temperature for the day \n",
    "    averageTemperatureUncertainty: The amout by which the temperature recording may be wrong\n",
    "    city: City where temperature was recorded\n",
    "    country: Country, always the US for this project\n",
    "    longitude: Longitdue of the city where temperature was recored\n",
    "    latitude: Longitdue of the city where temperature was recored\n",
    "\n",
    "\n",
    "dim_demographics: Provides population statistics on cities in the US. Grain is city/state/race.\n",
    "\n",
    "    city: City name\n",
    "    state: State city is in\n",
    "    median_age: Median age of residents of the city\n",
    "    male_pop: Number of men in the city\n",
    "    female_pop: Number of women in the city\n",
    "    total_pop: Total population of the city\n",
    "    num_vets: Number of veterans in the city\n",
    "    foreign_born: Number of foreign-born people in the city\n",
    "    avg_household_size: Average household size\n",
    "    state_code: Two-letter code for state\n",
    "    race: Primay race in the city: White, Hispanic or Latino, Asian, Black or African-American, or American Indian and Alaska Native\n",
    "    count: Number of people of that race in the city\n",
    "    \n",
    "dim_airport: Provides information about various airports in the US.\n",
    "\n",
    "    ident: Primay key\n",
    "    type: Type of airport\n",
    "    name: Name of airport\n",
    "    elevation_ft: Elevation of airport\n",
    "    continent: Continent where airport resides, always US for this project\n",
    "    iso_country: The ISO country code\n",
    "    iso_region: The ISO region code\n",
    "    municipality: Municipality where airport resides\n",
    "    gps_code: The GPS code for where the airport resides\n",
    "    iata_code: The IATA code\n",
    "    local_code: The local code\n",
    "    coordinates: Geographic coordinates of where the airport resides\n",
    "\n",
    "dim_port: A list of the ports of arrival\n",
    "\n",
    "    code: a short code\n",
    "    name: the name of the port; there are some No PORT Code ([code]) values too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "I utilized Python and Pandas because they provide simple functionality that can read csv files and insert the data into a relational database.  I also used Psycopg for reading and inserting json files.  I chose this method because it was the simplest method I could find for reading json file and also because I wanted to include more than 1 type of tool for reading and inserting data as part of this project.\n",
    "\n",
    "The data was structured and formatted well enough to make using a SQL relational database a good fit. I chose Postgres because it is fast and robust.\n",
    "\n",
    "The data should be updated once a month to coincide with the monthly immigration reports.  Perhaps some of the dimension tables could be updated less frequently.  For instance, new airports will not be added that often or charactersistics of the airport will not change.  However, since these are fairly quick sql procesess the montly update would not cause much stress on the overall system.\n",
    "\n",
    "If the data was increased by 100 times, we would need to make a fairly major overhaul to the approach. Instead of inserting data directly to tables I would first convert the data into a format that could be passed to S3.  I would then do a bulk copy of the table from S3 to the database.  Additional analysis would also need to be performed on both the size and number of processors.\n",
    "\n",
    "If the data were to populate a dashboard that must be updated every day, I would move the system to Airflow and modify the appropriate processes.\n",
    "\n",
    "There should not be a problem with 100 or so people accessing the data.  However, if there were issues with performance when many users are accessing the data we could implement Concurrency Scaling in AWS.  According to Amazon \"With the Concurrency Scaling feature, you can support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance.\".  Details can be found here: https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table for extracting data to\n",
    "!python load_immigration_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
